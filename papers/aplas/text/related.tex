\section{Related Work}

Specialization is an attractive technique aimed to improve the performance of a program if some of its arguments are known statically.
Specialization is studied for functional, imperative and logic programing and comes in different forms: partial evaluation~\cite{jonesbook} and partial deduction~\cite{lloyd1991partial}, supercompilation~\cite{soerensen1996positive}, distillation~\cite{hamilton2007distillation} and many more.


The heart of supercompilation-based techniques is \emph{driving}~---~a symbolic execution of a program through all possible execution paths.
The result of driving is so-called \emph{process tree} where nodes correspond to \emph{configurations} which represent computation state.
For example, in the case of pure functional programming languages, the computational state might be a term.
Each path in the tree corresponds to some concrete program execution.
The two main sources for supercompilation optimizations are aggressive information propagation about variables' values, equalities and disequalities, and precomputing of all deterministic semantic evaluation steps.
The latter process, also known as \emph{deforestation}~\cite{deforestation}, means  combining of consecutive process tree nodes with no branching.
When the tree is constructed, the resulting, or \emph{residual}, program can be extracted from the process tree by the process called \emph{residualization}.
Of course, process tree can contain infinite branches.
\emph{Whistles} --- heuristics to identify possibly infinite branches --- are used to ensure supercompilation terminates.
If a whistle signalls during the construction of some branch, then something should be done to ensure termination.
The most common approaches include stop driving the infinite branch completely (no specialization is done in this case and the source code is blindly copied into the residual program) or fold the process tree to a \emph{process graph}.
The main instrument to perform such a folding is \emph{generalization}.
Generalization, abstracting away some computed data about the current term, makes folding possible.
%  i.e. abstracting away some computed data about the current term which makes folding possible.
One source of infinite branches is consecutive recursive calls to the same function with an accumulating parameter: by unfolding such a call further one can only increase the term size which leads to nontermination.
The accumulating parameter can be removed by replacing the call with its generalization.
There are several ways to ensure process correctness and termination, most-specific generalization
(anti-unification) and using \emph{homeomorphic embedding}~\cite{Higman52,Kruskal60} as a
whistle being the most common.
%the most common being \emph{homeomorphic embedding}~\cite{Higman52,Kruskal60} used as a whistle and most-specific generalization of terms.
% When two dangerously similar nodes are encountered,
% For example, two consecutive recursive calls of a function with accumulating parameter, and the first is not an instance of the second, then one may construct a new, generalized, node such that both of original nodes are instances of the last one, then one of the initial nodes is replaced with the generalized one.

While supercompilation generally improves the behaviour of input programs and distillation can even provide superlinear speedup, there are no ways to predict the effect of specialization on a given program in the general.
What is worse, the efficiency of residual program from the target language evaluator point of view is rarely considered in the literature.
The main optimization source is computing in advance all possible intermediate and statically-known semantics steps at program transformation-time.
Other criteria, like the size of the generated program or possible optimizations and execution cost of different language constructions by the target language evaluator, are usually out of consideration~\cite{jonesbook}.
It is known that supercompilation may adversely affect GHC optimizations yielding standalone compilation more powerful~\cite{SCBE,TCES} and cause code explosion~\cite{SCHC}.
Moreover, it may be hard to predict the real speedup of any given program on concrete examples even disregarding the problems above because of the complexity of the transformation algorithm.
The worst-case for partial evaluation is when all static variables are used in a dynamic context, and there is some advice on how to implement a partial evaluator as well as a target program so that specialization indeed improves its performance~\cite{jonesbook,bulyonkov84}.
There is a lack of research in determining the classes of programs which transformers would definitely speed~up.

Conjunctive partial deduction~\cite{de1999conjunctive} makes an effort to provide reasonable control for the left-to-right evaluation strategy of \pro.
CPD constructs a tree which models goal evaluation and is similar to a SLDNF tree, then a residual program is generated from this tree.
Partial deduction itself resembles driving in supercompilation~\cite{gluck1994partial}.
The specialization is done in two levels of control: the local control determines the shape of the residual programs, while the global control ensures that every relation which can be called in the residual program is indeed defined.
The leaves of local control trees become nodes of the global control tree.
CPD analyses these nodes at the global level and runs local control for all those which are new.

At the local level, CPD examines a conjunction of atoms by considering each atom one-by-one from left to right.
An atom is \emph{unfolded} if it is deemed safe, i.e. a whistle based on homeomorphic embedding does not signal for the atom.
When an atom is unfolded, a clause whose head can be unified with the atom is found, and a new node is added into the tree where the atom in the conjunction is replaced with the body of that clause.
If there is more than one suitable head, then several branches are added into the tree which corresponds to the disjunction in the residualized program.
An adaptation of CPD for the \mk programming language is described in~\cite{lozov2019relational}.

\ecce partial deduction system~\cite{leuschel1997ecce} is the most mature implementation of CPD for \pro.
\ecce provides various implementations of both local and global control as well as several degrees of post-processing.
Unfortunately there is neither an automatic procedure to choose what control setting is likely to improve input programs the most nor any informal recommendations on how to choose the best settings.
The choice of the proper control is left to the user.

An empitical study shown that the most well-behaved strategy of local control in CPD for \pro is \emph{deterministic unfolding}~\cite{leuschel1997advanced}.
An atom is unfolded only if precisely one suitable clause head exists for it with the one exception: it is allowed to unfold an atom non-deterministically once for one local control tree.
This means that if a non-deterministic atom is the leftmost within a conjunction, it is most likely to be unfolded and to introduce many new relation calls within the conjunction.
We believe this is the core problem with CPD which limits its power when applied to \mk.
The strategy of unfolding atoms from left to right is reasonable in the context of \pro because it mimics the way programs in \pro execute.
But it often leads to larger global control trees and, as a result, bigger, less efficient programs.
The evaluation result of a \mk program does not depend on the order of atoms (relation calls) within a conjunction, thus we believe a better result can be achieved by selecting a relation call which can restrict the number of branches in the tree.
We describe our approach which implements this idea in the next section.
